{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n\n!apt update -y -qq\n!apt install -y -qq curl lshw libcairo2-dev pkg-config python3-dev\n!curl https://ollama.ai/install.sh | sh\n!pip install ollama\n!pip install openai\n!pip install gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Libs\n\nfrom ollama import chat\nfrom IPython.display import display, Markdown\nimport subprocess\nfrom openai import OpenAI\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T10:56:56.605388Z","iopub.execute_input":"2025-04-06T10:56:56.605627Z","iopub.status.idle":"2025-04-06T10:57:02.438735Z","shell.execute_reply.started":"2025-04-06T10:56:56.605605Z","shell.execute_reply":"2025-04-06T10:57:02.437793Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Serve and Pull Model\n\nsub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n!ollama pull gemma3:12b","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prepare message template\n\nMODEL = 'gemma3:12b'\nopenai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\nsystem_message = \"You are a helpful assistant. answer short in one line\"\n\ndef chat(message, history):\n    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n    response = \"\"\n    for chunk in stream:\n        response += chunk.choices[0].delta.content or ''\n        yield response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T11:06:43.651240Z","iopub.execute_input":"2025-04-06T11:06:43.651504Z","iopub.status.idle":"2025-04-06T11:06:43.744465Z","shell.execute_reply.started":"2025-04-06T11:06:43.651485Z","shell.execute_reply":"2025-04-06T11:06:43.743649Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Lunch chat interface\n\ngr.ChatInterface(fn=chat, type=\"messages\").launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}