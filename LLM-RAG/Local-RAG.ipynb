{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11314939,"sourceType":"datasetVersion","datasetId":7077406}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n\n!apt update -y -qq\n!apt install -y -qq curl lshw libcairo2-dev pkg-config python3-dev\n!pip install gradio\n!pip install -qU langchain langchain_community\n!pip install -qU langchain_chroma\n!pip install -qU langchain_ollama\n!pip install -qU beautifulsoup4\n!pip install -qU pypdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Libs\n\nimport os\nimport glob\nimport subprocess\nfrom dotenv import load_dotenv\nimport gradio as gr\nfrom langchain_community.document_loaders import TextLoader, DirectoryLoader\nfrom langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_ollama import OllamaEmbeddings\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:08:13.461220Z","iopub.execute_input":"2025-04-07T21:08:13.461582Z","iopub.status.idle":"2025-04-07T21:08:17.441640Z","shell.execute_reply.started":"2025-04-07T21:08:13.461553Z","shell.execute_reply":"2025-04-07T21:08:17.440568Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Run Ollama\n\nsub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n!ollama pull nomic-embed-text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fetch Knowledge Base and convert it to chunks of data\n\nfolders = glob.glob(\"/kaggle/input/rag-kb/RAG Knowledge Base/*\")\n\ndef add_metadata(doc, doc_type):\n    doc.metadata[\"doc_type\"] = doc_type\n    return doc\n\ntext_loader_kwargs = {'encoding': 'utf-8'}\n\ndocuments = []\nfor folder in folders:\n    doc_type = os.path.basename(folder)\n    loader = DirectoryLoader(folder, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n    folder_docs = loader.load()\n    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n\ntext_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=400)\nchunks = text_splitter.split_documents(documents)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:16:16.773157Z","iopub.execute_input":"2025-04-07T21:16:16.773493Z","iopub.status.idle":"2025-04-07T21:16:16.813183Z","shell.execute_reply.started":"2025-04-07T21:16:16.773468Z","shell.execute_reply":"2025-04-07T21:16:16.812533Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Create embedding vector and store it by chroma\n\nDB_NAME = \"vector_db\"\n\nembeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n\n# Delete if already exists\nif os.path.exists(DB_NAME):\n    Chroma(persist_directory=DB_NAME, embedding_function=embeddings).delete_collection()\n\n# Create vectorstore\nvectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=DB_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T21:16:22.758910Z","iopub.execute_input":"2025-04-07T21:16:22.759213Z","iopub.status.idle":"2025-04-07T21:16:23.658895Z","shell.execute_reply.started":"2025-04-07T21:16:22.759190Z","shell.execute_reply":"2025-04-07T21:16:23.658282Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Setup Langchain\n\nMODEL = \"gemma3:12b\"\n!ollama pull gemma3:12b\nllm = ChatOllama(temperature=0.7, model=MODEL)\n\n# set up the conversation memory for the chat\nmemory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n\n# the retriever is an abstraction over the VectorStore that will be used during RAG\nretriever = vectorstore.as_retriever()\n\n# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\nconversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lunch chat interface\n\ndef chat(question, history):\n    result = conversation_chain.invoke({\"question\": question})\n    return result[\"answer\"]\nview = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}